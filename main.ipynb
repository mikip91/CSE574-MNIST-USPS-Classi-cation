{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import mode\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,)\n",
      "(10000, 784) (10000,)\n",
      "(10000, 784) (10000,)\n",
      "(60000, 784) (60000,)\n"
     ]
    }
   ],
   "source": [
    "filename = 'mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "print(training_data[0].shape, training_data[1].shape)\n",
    "print(validation_data[0].shape, validation_data[1].shape)\n",
    "print(test_data[0].shape, test_data[1].shape)\n",
    "final_data = np.r_[training_data[0], validation_data[0]]\n",
    "final_data_target= np.append(training_data[1], validation_data[1])\n",
    "print(final_data.shape, final_data_target.shape)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_voting_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 1: Softmax Regression Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z, epsilon=1e-9):\n",
    "    e = np.exp(Z - np.max(Z))\n",
    "    if e.ndim == 1:\n",
    "        return e / np.sum(e, axis=0) + epsilon\n",
    "    else:  \n",
    "        return e / np.array([np.sum(e, axis=1)]).T + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(W, X):\n",
    "    X_ones = np.hstack((X, np.ones(((X.shape[0]), 1))))\n",
    "    XW = np.dot(X_ones, W)\n",
    "    smax = softmax(XW)\n",
    "    return smax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-2\n",
    "def one_hot_encode(labels_list, max_number):\n",
    "    samples_number = len(labels_list)\n",
    "    b = np.zeros((samples_number, max_number))\n",
    "    b[np.arange(samples_number), labels_list] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(W, X, Y):\n",
    "    m = X.shape[0]\n",
    "    Y_tilde = infer(W, X)    \n",
    "    return (-1 / m) * np.sum(np.log(Y_tilde) * Y) + eta / 2 * np.sum(W * W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_onehot = one_hot_encode(final_data_target, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(W, X, Y):   \n",
    "    X_alt = np.hstack((X, np.ones(((X.shape[0]), 1))))\n",
    "    m = X.shape[0]\n",
    "    Y_tilde = infer(W, X)   \n",
    "    return (-1 / m) * np.dot(X_alt.T, (Y - Y_tilde)) + eta * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=10\n",
    "def train(X_train, y_train, batch_size=512, num_epoch=256, n_classes=n_classes, step=1e-3, plot_loss=True):\n",
    "    losses = []\n",
    "    n_features = X_train.shape[1]\n",
    "    w = np.random.randn(n_features+1, n_classes)/n_features\n",
    "    for epoch in range(num_epoch):        \n",
    "        for iter_num, (x_batch, y_batch) in enumerate(zip(np.split(X_train, batch_size), np.split(y_train, batch_size))):\n",
    "            grad = get_grad(w, x_batch, one_hot_encode(y_batch, n_classes))\n",
    "            gradient_step = step * grad\n",
    "            w -= gradient_step\n",
    "            losses.append(loss(w, x_batch, one_hot_encode(y_batch, n_classes)))\n",
    "            \n",
    "    if plot_loss:\n",
    "        plt.plot(losses)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.show()    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(X, W):\n",
    "    probability_matrix = infer(W, X)\n",
    "    return np.array([np.argmax(t) for t in probability_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 10)\n"
     ]
    }
   ],
   "source": [
    "W = train(final_data, final_data_target, num_epoch=1000)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (1, 10000)\n",
      "Errors: 1025  Correct :8975\n",
      "Implementation 1:Testing Accuracy for Softmax Regression with MNIST data: 89.75\n",
      "Implementation 1:Confusion Matrix for Softmax Regression with MNIST data:\n",
      "[[ 960    0    2    3    0    0    7    1    7    0]\n",
      " [   0 1103    2    4    1    2    4    0   19    0]\n",
      " [  13   12  879   19   18    0   19   22   43    7]\n",
      " [   6    1   17  899    1   30    6   15   22   13]\n",
      " [   1    6    5    0  898    1   11    1    8   51]\n",
      " [  16    8    5   47   16  721   17   10   41   11]\n",
      " [  16    3    7    2   13   17  895    1    4    0]\n",
      " [   3   21   30    4   11    0    0  914    4   41]\n",
      " [  10   11   12   31   11   26   13   13  830   17]\n",
      " [  14    8    8    9   47   15    0   24    8  876]]\n"
     ]
    }
   ],
   "source": [
    "right=0\n",
    "wrong=0\n",
    "y_pred = make_prediction(test_data[0], W)\n",
    "\n",
    "majority_voting_list.append(y_pred)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_list).shape))\n",
    "for i,j in zip(y_pred,test_data[1]): \n",
    "    if j == i:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "print(\"Implementation 1:Testing Accuracy for Softmax Regression with MNIST data: \" + str(right/(right+wrong)*100))\n",
    "print(\"Implementation 1:Confusion Matrix for Softmax Regression with MNIST data:\")\n",
    "print(confusion_matrix(test_data[1], (y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 2: Softmax Regression Using Solver as `Ibfgs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_lr = linear_model.LogisticRegression(multi_class='multinomial',solver ='lbfgs').fit(final_data, final_data_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (2, 10000)\n",
      "Implementation 2:Testing Accuracy for Softmax Regression with MNIST data: 0.9261\n",
      "Implementation 2:Confusion Matrix for Softmax Regression with MNIST data:\n",
      "[[ 958    0    0    3    1    8    5    4    1    0]\n",
      " [   0 1111    4    2    0    2    3    2   11    0]\n",
      " [   5    7  932   15   10    3   14    7   35    4]\n",
      " [   4    1   18  917    1   22    4   11   24    8]\n",
      " [   1    1    7    3  917    0   10    4    8   31]\n",
      " [  11    2    2   34   11  780   14    5   29    4]\n",
      " [   9    3    8    2    8   14  911    2    1    0]\n",
      " [   1    7   26    4    7    1    0  950    3   29]\n",
      " [   9   12    8   22    7   25   12    6  859   14]\n",
      " [  10    8    1    9   23    6    0   19    7  926]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = accuracy_score(test_data[1], mul_lr.predict(test_data[0]))\n",
    "confusion_matrix_logisticReg2 = confusion_matrix(test_data[1], mul_lr.predict(test_data[0]))\n",
    "majority_voting_list.append(mul_lr.predict(test_data[0]))\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_list).shape))\n",
    "print(\"Implementation 2:Testing Accuracy for Softmax Regression with MNIST data: \"+ str(y_pred))\n",
    "print(\"Implementation 2:Confusion Matrix for Softmax Regression with MNIST data:\")\n",
    "print(confusion_matrix_logisticReg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 3: Softmax Regression Using Solver as `Ibfgs` , Penalty as `l2` and `warm_start` as true "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_lr2 = linear_model.LogisticRegression(multi_class='multinomial',solver ='lbfgs',penalty ='l2',C=1e5, warm_start=True).fit(final_data, final_data_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (3, 10000)\n",
      "Implementation 3:Testing Accuracy for softmax regression with MNIST data: 0.9257\n",
      "Implementation 3:Confusion Matrix for Softmax Regression with MNIST data:\n",
      "[[ 960    0    0    2    1    7    5    4    1    0]\n",
      " [   0 1111    4    2    0    2    3    2   11    0]\n",
      " [   5    7  927   17    8    3   16    6   39    4]\n",
      " [   4    1   19  916    1   22    4   11   23    9]\n",
      " [   1    1    7    3  914    0    9    8    8   31]\n",
      " [  11    2    3   32    8  781   15    5   30    5]\n",
      " [   9    3    8    2    7   14  912    2    1    0]\n",
      " [   1    7   25    5    7    1    0  952    3   27]\n",
      " [   8   12    8   24    7   25   12    7  858   13]\n",
      " [   8    8    0   11   23    6    0   19    8  926]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = accuracy_score(test_data[1], mul_lr2.predict(test_data[0]))\n",
    "confusion_matrix_logisticReg3 = confusion_matrix(test_data[1], mul_lr2.predict(test_data[0]))\n",
    "majority_voting_list.append(mul_lr2.predict(test_data[0]))\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_list).shape))\n",
    "print(\"Implementation 3:Testing Accuracy for softmax regression with MNIST data: \"+str(y_pred))\n",
    "print(\"Implementation 3:Confusion Matrix for Softmax Regression with MNIST data:\")\n",
    "print(confusion_matrix_logisticReg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 4: Neural Network Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/256\n",
      "54000/54000 [==============================] - 3s 64us/step - loss: 2.1239 - acc: 0.4633 - val_loss: 1.9002 - val_acc: 0.7157\n",
      "Epoch 2/256\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 1.7505 - acc: 0.7138 - val_loss: 1.5570 - val_acc: 0.7717\n",
      "Epoch 3/256\n",
      "54000/54000 [==============================] - 1s 26us/step - loss: 1.4525 - acc: 0.7523 - val_loss: 1.2802 - val_acc: 0.8075\n",
      "Epoch 4/256\n",
      "54000/54000 [==============================] - 1s 26us/step - loss: 1.2217 - acc: 0.7811 - val_loss: 1.0734 - val_acc: 0.8320\n",
      "Epoch 5/256\n",
      "54000/54000 [==============================] - 1s 23us/step - loss: 1.0525 - acc: 0.8047 - val_loss: 0.9236 - val_acc: 0.8510\n",
      "Epoch 6/256\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.9291 - acc: 0.8199 - val_loss: 0.8138 - val_acc: 0.8633\n",
      "Epoch 7/256\n",
      "54000/54000 [==============================] - 1s 23us/step - loss: 0.8370 - acc: 0.8325 - val_loss: 0.7307 - val_acc: 0.8707\n",
      "Epoch 8/256\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.7663 - acc: 0.8423 - val_loss: 0.6665 - val_acc: 0.8755\n",
      "Epoch 9/256\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.7107 - acc: 0.8495 - val_loss: 0.6155 - val_acc: 0.8838\n",
      "Epoch 10/256\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.6657 - acc: 0.8561 - val_loss: 0.5743 - val_acc: 0.8883\n",
      "Epoch 11/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.6286 - acc: 0.8608 - val_loss: 0.5400 - val_acc: 0.8910\n",
      "Epoch 12/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.5976 - acc: 0.8651 - val_loss: 0.5113 - val_acc: 0.8960\n",
      "Epoch 13/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.5711 - acc: 0.8682 - val_loss: 0.4870 - val_acc: 0.8982\n",
      "Epoch 14/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.5484 - acc: 0.8716 - val_loss: 0.4663 - val_acc: 0.8993\n",
      "Epoch 15/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.5287 - acc: 0.8741 - val_loss: 0.4481 - val_acc: 0.9022\n",
      "Epoch 16/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.5113 - acc: 0.8770 - val_loss: 0.4322 - val_acc: 0.9033\n",
      "Epoch 17/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.4960 - acc: 0.8791 - val_loss: 0.4183 - val_acc: 0.9050\n",
      "Epoch 18/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.4823 - acc: 0.8814 - val_loss: 0.4059 - val_acc: 0.9070\n",
      "Epoch 19/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.4701 - acc: 0.8830 - val_loss: 0.3947 - val_acc: 0.9087\n",
      "Epoch 20/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.4590 - acc: 0.8848 - val_loss: 0.3848 - val_acc: 0.9097\n",
      "Epoch 21/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.4489 - acc: 0.8865 - val_loss: 0.3757 - val_acc: 0.9117\n",
      "Epoch 22/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.4398 - acc: 0.8880 - val_loss: 0.3675 - val_acc: 0.9132\n",
      "Epoch 23/256\n",
      "54000/54000 [==============================] - 1s 20us/step - loss: 0.4314 - acc: 0.8889 - val_loss: 0.3603 - val_acc: 0.9130\n",
      "Epoch 24/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.4238 - acc: 0.8901 - val_loss: 0.3532 - val_acc: 0.9142\n",
      "Epoch 25/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.4166 - acc: 0.8914 - val_loss: 0.3469 - val_acc: 0.9150\n",
      "Epoch 26/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.4100 - acc: 0.8924 - val_loss: 0.3412 - val_acc: 0.9160\n",
      "Epoch 27/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.4039 - acc: 0.8933 - val_loss: 0.3358 - val_acc: 0.9167\n",
      "Epoch 28/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3982 - acc: 0.8943 - val_loss: 0.3307 - val_acc: 0.9180\n",
      "Epoch 29/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.3929 - acc: 0.8954 - val_loss: 0.3260 - val_acc: 0.9180\n",
      "Epoch 30/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3879 - acc: 0.8961 - val_loss: 0.3216 - val_acc: 0.9198\n",
      "Epoch 31/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.3832 - acc: 0.8968 - val_loss: 0.3175 - val_acc: 0.9213\n",
      "Epoch 32/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.3787 - acc: 0.8979 - val_loss: 0.3138 - val_acc: 0.9212\n",
      "Epoch 33/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.3745 - acc: 0.8984 - val_loss: 0.3100 - val_acc: 0.9227\n",
      "Epoch 34/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3705 - acc: 0.8994 - val_loss: 0.3066 - val_acc: 0.9233\n",
      "Epoch 35/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.3667 - acc: 0.8999 - val_loss: 0.3034 - val_acc: 0.9237\n",
      "Epoch 36/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.3631 - acc: 0.9005 - val_loss: 0.3004 - val_acc: 0.9237\n",
      "Epoch 37/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.3597 - acc: 0.9013 - val_loss: 0.2974 - val_acc: 0.9235\n",
      "Epoch 38/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.3564 - acc: 0.9021 - val_loss: 0.2944 - val_acc: 0.9245\n",
      "Epoch 39/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.3533 - acc: 0.9025 - val_loss: 0.2918 - val_acc: 0.9253\n",
      "Epoch 40/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3502 - acc: 0.9031 - val_loss: 0.2896 - val_acc: 0.9248\n",
      "Epoch 41/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.3474 - acc: 0.9037 - val_loss: 0.2869 - val_acc: 0.9258\n",
      "Epoch 42/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.3446 - acc: 0.9044 - val_loss: 0.2846 - val_acc: 0.9258\n",
      "Epoch 43/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.3419 - acc: 0.9050 - val_loss: 0.2823 - val_acc: 0.9262\n",
      "Epoch 44/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.3394 - acc: 0.9055 - val_loss: 0.2801 - val_acc: 0.9260\n",
      "Epoch 45/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.3369 - acc: 0.9059 - val_loss: 0.2781 - val_acc: 0.9262\n",
      "Epoch 46/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.3345 - acc: 0.9065 - val_loss: 0.2761 - val_acc: 0.9265\n",
      "Epoch 47/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.3322 - acc: 0.9069 - val_loss: 0.2742 - val_acc: 0.9265\n",
      "Epoch 48/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.3300 - acc: 0.9075 - val_loss: 0.2722 - val_acc: 0.9263\n",
      "Epoch 49/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.3278 - acc: 0.9079 - val_loss: 0.2706 - val_acc: 0.9272\n",
      "Epoch 50/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.3257 - acc: 0.9081 - val_loss: 0.2688 - val_acc: 0.9273\n",
      "Epoch 51/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.3237 - acc: 0.9089 - val_loss: 0.2672 - val_acc: 0.9278\n",
      "Epoch 52/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.3217 - acc: 0.9095 - val_loss: 0.2655 - val_acc: 0.9278\n",
      "Epoch 53/256\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.3198 - acc: 0.9100 - val_loss: 0.2640 - val_acc: 0.9283\n",
      "Epoch 54/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.3179 - acc: 0.9103 - val_loss: 0.2626 - val_acc: 0.9285\n",
      "Epoch 55/256\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.3160 - acc: 0.9109 - val_loss: 0.2608 - val_acc: 0.9295\n",
      "Epoch 56/256\n",
      "54000/54000 [==============================] - ETA: 0s - loss: 0.3145 - acc: 0.911 - 1s 18us/step - loss: 0.3143 - acc: 0.9114 - val_loss: 0.2593 - val_acc: 0.9288\n",
      "Epoch 57/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.3126 - acc: 0.9115 - val_loss: 0.2580 - val_acc: 0.9292\n",
      "Epoch 58/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.3109 - acc: 0.9121 - val_loss: 0.2566 - val_acc: 0.9295\n",
      "Epoch 59/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3093 - acc: 0.9124 - val_loss: 0.2552 - val_acc: 0.9302\n",
      "Epoch 60/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3076 - acc: 0.9129 - val_loss: 0.2539 - val_acc: 0.9308\n",
      "Epoch 61/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.9134 - val_loss: 0.2526 - val_acc: 0.9313\n",
      "Epoch 62/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.9136 - val_loss: 0.2512 - val_acc: 0.9317\n",
      "Epoch 63/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.3030 - acc: 0.9142 - val_loss: 0.2500 - val_acc: 0.9317\n",
      "Epoch 64/256\n",
      "54000/54000 [==============================] - 1s 20us/step - loss: 0.3015 - acc: 0.9146 - val_loss: 0.2490 - val_acc: 0.9318\n",
      "Epoch 65/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.3001 - acc: 0.9149 - val_loss: 0.2477 - val_acc: 0.9328\n",
      "Epoch 66/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2987 - acc: 0.9153 - val_loss: 0.2464 - val_acc: 0.9332\n",
      "Epoch 67/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2973 - acc: 0.9157 - val_loss: 0.2454 - val_acc: 0.9337\n",
      "Epoch 68/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2959 - acc: 0.9164 - val_loss: 0.2443 - val_acc: 0.9337\n",
      "Epoch 69/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2946 - acc: 0.9167 - val_loss: 0.2433 - val_acc: 0.9340\n",
      "Epoch 70/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2932 - acc: 0.9170 - val_loss: 0.2423 - val_acc: 0.9340\n",
      "Epoch 71/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2920 - acc: 0.9171 - val_loss: 0.2411 - val_acc: 0.9347\n",
      "Epoch 72/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2907 - acc: 0.9176 - val_loss: 0.2401 - val_acc: 0.9342\n",
      "Epoch 73/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2894 - acc: 0.9177 - val_loss: 0.2391 - val_acc: 0.9350\n",
      "Epoch 74/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2882 - acc: 0.9182 - val_loss: 0.2382 - val_acc: 0.9353\n",
      "Epoch 75/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2870 - acc: 0.9184 - val_loss: 0.2372 - val_acc: 0.9357\n",
      "Epoch 76/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2858 - acc: 0.9188 - val_loss: 0.2362 - val_acc: 0.9357\n",
      "Epoch 77/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2846 - acc: 0.9190 - val_loss: 0.2353 - val_acc: 0.9360\n",
      "Epoch 78/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2835 - acc: 0.9194 - val_loss: 0.2344 - val_acc: 0.9362\n",
      "Epoch 79/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2823 - acc: 0.9196 - val_loss: 0.2333 - val_acc: 0.9363\n",
      "Epoch 80/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2812 - acc: 0.9196 - val_loss: 0.2326 - val_acc: 0.9367\n",
      "Epoch 81/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2801 - acc: 0.9201 - val_loss: 0.2317 - val_acc: 0.9373\n",
      "Epoch 82/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2790 - acc: 0.9203 - val_loss: 0.2307 - val_acc: 0.9370\n",
      "Epoch 83/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2779 - acc: 0.9208 - val_loss: 0.2299 - val_acc: 0.9372\n",
      "Epoch 84/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2769 - acc: 0.9211 - val_loss: 0.2290 - val_acc: 0.9368\n",
      "Epoch 85/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2758 - acc: 0.9214 - val_loss: 0.2284 - val_acc: 0.9378\n",
      "Epoch 86/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2748 - acc: 0.9214 - val_loss: 0.2276 - val_acc: 0.9378\n",
      "Epoch 87/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2738 - acc: 0.9218 - val_loss: 0.2264 - val_acc: 0.9382\n",
      "Epoch 88/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2728 - acc: 0.9220 - val_loss: 0.2258 - val_acc: 0.9377\n",
      "Epoch 89/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2718 - acc: 0.9222 - val_loss: 0.2249 - val_acc: 0.9387\n",
      "Epoch 90/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2708 - acc: 0.9225 - val_loss: 0.2242 - val_acc: 0.9388\n",
      "Epoch 91/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2698 - acc: 0.9231 - val_loss: 0.2233 - val_acc: 0.9393\n",
      "Epoch 92/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2688 - acc: 0.9232 - val_loss: 0.2226 - val_acc: 0.9392\n",
      "Epoch 93/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2679 - acc: 0.9234 - val_loss: 0.2218 - val_acc: 0.9393\n",
      "Epoch 94/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2669 - acc: 0.9235 - val_loss: 0.2212 - val_acc: 0.9395\n",
      "Epoch 95/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2660 - acc: 0.9241 - val_loss: 0.2204 - val_acc: 0.9398\n",
      "Epoch 96/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2651 - acc: 0.9239 - val_loss: 0.2196 - val_acc: 0.9395\n",
      "Epoch 97/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2642 - acc: 0.9243 - val_loss: 0.2188 - val_acc: 0.9400\n",
      "Epoch 98/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2632 - acc: 0.9246 - val_loss: 0.2184 - val_acc: 0.9393\n",
      "Epoch 99/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2624 - acc: 0.9249 - val_loss: 0.2176 - val_acc: 0.9400\n",
      "Epoch 100/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2615 - acc: 0.9251 - val_loss: 0.2168 - val_acc: 0.9405\n",
      "Epoch 101/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2606 - acc: 0.9255 - val_loss: 0.2162 - val_acc: 0.9400\n",
      "Epoch 102/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2598 - acc: 0.9258 - val_loss: 0.2155 - val_acc: 0.9408\n",
      "Epoch 103/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2589 - acc: 0.9259 - val_loss: 0.2148 - val_acc: 0.9410\n",
      "Epoch 104/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2581 - acc: 0.9261 - val_loss: 0.2140 - val_acc: 0.9412\n",
      "Epoch 105/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2572 - acc: 0.9264 - val_loss: 0.2133 - val_acc: 0.9412\n",
      "Epoch 106/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2564 - acc: 0.9269 - val_loss: 0.2127 - val_acc: 0.9407\n",
      "Epoch 107/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2556 - acc: 0.9269 - val_loss: 0.2121 - val_acc: 0.9408\n",
      "Epoch 108/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2548 - acc: 0.9273 - val_loss: 0.2114 - val_acc: 0.9412\n",
      "Epoch 109/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2540 - acc: 0.9276 - val_loss: 0.2107 - val_acc: 0.9422\n",
      "Epoch 110/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2532 - acc: 0.9277 - val_loss: 0.2101 - val_acc: 0.9423\n",
      "Epoch 111/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2524 - acc: 0.9279 - val_loss: 0.2095 - val_acc: 0.9423\n",
      "Epoch 112/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2516 - acc: 0.9281 - val_loss: 0.2089 - val_acc: 0.9428\n",
      "Epoch 113/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2508 - acc: 0.9281 - val_loss: 0.2082 - val_acc: 0.9428\n",
      "Epoch 114/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2501 - acc: 0.9285 - val_loss: 0.2078 - val_acc: 0.9427\n",
      "Epoch 115/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2493 - acc: 0.9286 - val_loss: 0.2070 - val_acc: 0.9432\n",
      "Epoch 116/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2485 - acc: 0.9290 - val_loss: 0.2067 - val_acc: 0.9430\n",
      "Epoch 117/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2478 - acc: 0.9289 - val_loss: 0.2060 - val_acc: 0.9432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2470 - acc: 0.9291 - val_loss: 0.2052 - val_acc: 0.9433\n",
      "Epoch 119/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2463 - acc: 0.9292 - val_loss: 0.2046 - val_acc: 0.9435\n",
      "Epoch 120/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2456 - acc: 0.9296 - val_loss: 0.2040 - val_acc: 0.9438\n",
      "Epoch 121/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2449 - acc: 0.9299 - val_loss: 0.2036 - val_acc: 0.9435\n",
      "Epoch 122/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2441 - acc: 0.9301 - val_loss: 0.2029 - val_acc: 0.9440\n",
      "Epoch 123/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2434 - acc: 0.9303 - val_loss: 0.2024 - val_acc: 0.9443\n",
      "Epoch 124/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2427 - acc: 0.9307 - val_loss: 0.2018 - val_acc: 0.9440\n",
      "Epoch 125/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2420 - acc: 0.9307 - val_loss: 0.2013 - val_acc: 0.9445\n",
      "Epoch 126/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2413 - acc: 0.9309 - val_loss: 0.2008 - val_acc: 0.9442\n",
      "Epoch 127/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2406 - acc: 0.9311 - val_loss: 0.2003 - val_acc: 0.9445\n",
      "Epoch 128/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2399 - acc: 0.9314 - val_loss: 0.1997 - val_acc: 0.9452\n",
      "Epoch 129/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2393 - acc: 0.9314 - val_loss: 0.1991 - val_acc: 0.9452\n",
      "Epoch 130/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2386 - acc: 0.9318 - val_loss: 0.1987 - val_acc: 0.9448\n",
      "Epoch 131/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2379 - acc: 0.9318 - val_loss: 0.1982 - val_acc: 0.9450\n",
      "Epoch 132/256\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2372 - acc: 0.9320 - val_loss: 0.1975 - val_acc: 0.9460\n",
      "Epoch 133/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2366 - acc: 0.9321 - val_loss: 0.1969 - val_acc: 0.9457\n",
      "Epoch 134/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2360 - acc: 0.9322 - val_loss: 0.1964 - val_acc: 0.9467\n",
      "Epoch 135/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2353 - acc: 0.9326 - val_loss: 0.1960 - val_acc: 0.9460\n",
      "Epoch 136/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2347 - acc: 0.9327 - val_loss: 0.1954 - val_acc: 0.9468\n",
      "Epoch 137/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2340 - acc: 0.9329 - val_loss: 0.1951 - val_acc: 0.9463\n",
      "Epoch 138/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2334 - acc: 0.9330 - val_loss: 0.1945 - val_acc: 0.9463\n",
      "Epoch 139/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2328 - acc: 0.9332 - val_loss: 0.1940 - val_acc: 0.9467\n",
      "Epoch 140/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2321 - acc: 0.9334 - val_loss: 0.1936 - val_acc: 0.9475\n",
      "Epoch 141/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2315 - acc: 0.9335 - val_loss: 0.1930 - val_acc: 0.9472\n",
      "Epoch 142/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2309 - acc: 0.9337 - val_loss: 0.1926 - val_acc: 0.9470\n",
      "Epoch 143/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2303 - acc: 0.9337 - val_loss: 0.1922 - val_acc: 0.9465\n",
      "Epoch 144/256\n",
      "54000/54000 [==============================] - ETA: 0s - loss: 0.2295 - acc: 0.934 - 1s 17us/step - loss: 0.2297 - acc: 0.9342 - val_loss: 0.1917 - val_acc: 0.9473\n",
      "Epoch 145/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2291 - acc: 0.9343 - val_loss: 0.1912 - val_acc: 0.9475\n",
      "Epoch 146/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2285 - acc: 0.9348 - val_loss: 0.1907 - val_acc: 0.9478\n",
      "Epoch 147/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2279 - acc: 0.9348 - val_loss: 0.1904 - val_acc: 0.9475\n",
      "Epoch 148/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2273 - acc: 0.9349 - val_loss: 0.1898 - val_acc: 0.9478\n",
      "Epoch 149/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2267 - acc: 0.9351 - val_loss: 0.1893 - val_acc: 0.9483\n",
      "Epoch 150/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2262 - acc: 0.9354 - val_loss: 0.1889 - val_acc: 0.9482\n",
      "Epoch 151/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2256 - acc: 0.9353 - val_loss: 0.1884 - val_acc: 0.9488\n",
      "Epoch 152/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2250 - acc: 0.9355 - val_loss: 0.1879 - val_acc: 0.9487\n",
      "Epoch 153/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2245 - acc: 0.9359 - val_loss: 0.1875 - val_acc: 0.9488\n",
      "Epoch 154/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2239 - acc: 0.9360 - val_loss: 0.1871 - val_acc: 0.9488\n",
      "Epoch 155/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2233 - acc: 0.9361 - val_loss: 0.1868 - val_acc: 0.9490\n",
      "Epoch 156/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2228 - acc: 0.9363 - val_loss: 0.1863 - val_acc: 0.9487\n",
      "Epoch 157/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2222 - acc: 0.9364 - val_loss: 0.1858 - val_acc: 0.9495\n",
      "Epoch 158/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2217 - acc: 0.9365 - val_loss: 0.1856 - val_acc: 0.9490\n",
      "Epoch 159/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2211 - acc: 0.9369 - val_loss: 0.1851 - val_acc: 0.9495\n",
      "Epoch 160/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2206 - acc: 0.9370 - val_loss: 0.1846 - val_acc: 0.9495\n",
      "Epoch 161/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2201 - acc: 0.9372 - val_loss: 0.1842 - val_acc: 0.9498\n",
      "Epoch 162/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2195 - acc: 0.9374 - val_loss: 0.1839 - val_acc: 0.9495\n",
      "Epoch 163/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2190 - acc: 0.9375 - val_loss: 0.1834 - val_acc: 0.9498\n",
      "Epoch 164/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2185 - acc: 0.9377 - val_loss: 0.1830 - val_acc: 0.9497\n",
      "Epoch 165/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2180 - acc: 0.9378 - val_loss: 0.1826 - val_acc: 0.9500\n",
      "Epoch 166/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2175 - acc: 0.9378 - val_loss: 0.1822 - val_acc: 0.9498\n",
      "Epoch 167/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2170 - acc: 0.9380 - val_loss: 0.1819 - val_acc: 0.9505\n",
      "Epoch 168/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2164 - acc: 0.9383 - val_loss: 0.1813 - val_acc: 0.9503\n",
      "Epoch 169/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2159 - acc: 0.9383 - val_loss: 0.1811 - val_acc: 0.9507\n",
      "Epoch 170/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2155 - acc: 0.9384 - val_loss: 0.1806 - val_acc: 0.9505\n",
      "Epoch 171/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2149 - acc: 0.9386 - val_loss: 0.1801 - val_acc: 0.9508\n",
      "Epoch 172/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2144 - acc: 0.9389 - val_loss: 0.1801 - val_acc: 0.9510\n",
      "Epoch 173/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2140 - acc: 0.9386 - val_loss: 0.1795 - val_acc: 0.9513\n",
      "Epoch 174/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2135 - acc: 0.9390 - val_loss: 0.1792 - val_acc: 0.9512\n",
      "Epoch 175/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2130 - acc: 0.9391 - val_loss: 0.1789 - val_acc: 0.9513\n",
      "Epoch 176/256\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2125 - acc: 0.9392 - val_loss: 0.1784 - val_acc: 0.9512\n",
      "Epoch 177/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2120 - acc: 0.9393 - val_loss: 0.1781 - val_acc: 0.9515\n",
      "Epoch 178/256\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2116 - acc: 0.9393 - val_loss: 0.1778 - val_acc: 0.9517\n",
      "Epoch 179/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2111 - acc: 0.9396 - val_loss: 0.1774 - val_acc: 0.9523\n",
      "Epoch 180/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2106 - acc: 0.9396 - val_loss: 0.1769 - val_acc: 0.9518\n",
      "Epoch 181/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2102 - acc: 0.9400 - val_loss: 0.1768 - val_acc: 0.9517\n",
      "Epoch 182/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2097 - acc: 0.9399 - val_loss: 0.1765 - val_acc: 0.9522\n",
      "Epoch 183/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2092 - acc: 0.9399 - val_loss: 0.1762 - val_acc: 0.9520\n",
      "Epoch 184/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2088 - acc: 0.9404 - val_loss: 0.1759 - val_acc: 0.9520\n",
      "Epoch 185/256\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2083 - acc: 0.9402 - val_loss: 0.1753 - val_acc: 0.9527\n",
      "Epoch 186/256\n",
      "54000/54000 [==============================] - 1s 19us/step - loss: 0.2079 - acc: 0.9404 - val_loss: 0.1750 - val_acc: 0.9530\n",
      "Epoch 187/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2074 - acc: 0.9407 - val_loss: 0.1747 - val_acc: 0.9525\n",
      "Epoch 188/256\n",
      "54000/54000 [==============================] - 1s 16us/step - loss: 0.2070 - acc: 0.9408 - val_loss: 0.1745 - val_acc: 0.9532\n",
      "Epoch 189/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2066 - acc: 0.9409 - val_loss: 0.1742 - val_acc: 0.9532\n",
      "Epoch 190/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2061 - acc: 0.9411 - val_loss: 0.1740 - val_acc: 0.9527\n",
      "Epoch 191/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2057 - acc: 0.9411 - val_loss: 0.1734 - val_acc: 0.9528\n",
      "Epoch 192/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2053 - acc: 0.9413 - val_loss: 0.1730 - val_acc: 0.9530\n",
      "Epoch 193/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2048 - acc: 0.9413 - val_loss: 0.1729 - val_acc: 0.9533\n",
      "Epoch 194/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2044 - acc: 0.9414 - val_loss: 0.1725 - val_acc: 0.9535\n",
      "Epoch 195/256\n",
      "54000/54000 [==============================] - 1s 18us/step - loss: 0.2040 - acc: 0.9416 - val_loss: 0.1722 - val_acc: 0.9532\n",
      "Epoch 196/256\n",
      "54000/54000 [==============================] - 1s 17us/step - loss: 0.2035 - acc: 0.9416 - val_loss: 0.1720 - val_acc: 0.9532\n",
      "Epoch 197/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2031 - acc: 0.9419 - val_loss: 0.1718 - val_acc: 0.9535\n",
      "Epoch 198/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2027 - acc: 0.9421 - val_loss: 0.1713 - val_acc: 0.9533\n",
      "Epoch 199/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2023 - acc: 0.9420 - val_loss: 0.1710 - val_acc: 0.9533\n",
      "Epoch 200/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.2019 - acc: 0.9422 - val_loss: 0.1706 - val_acc: 0.9537\n",
      "Epoch 201/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2015 - acc: 0.9422 - val_loss: 0.1705 - val_acc: 0.9535\n",
      "Epoch 202/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.2011 - acc: 0.9426 - val_loss: 0.1703 - val_acc: 0.9538\n",
      "Epoch 203/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2007 - acc: 0.9426 - val_loss: 0.1697 - val_acc: 0.9540\n",
      "Epoch 204/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.2002 - acc: 0.9427 - val_loss: 0.1696 - val_acc: 0.9542\n",
      "Epoch 205/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1999 - acc: 0.9429 - val_loss: 0.1693 - val_acc: 0.9538\n",
      "Epoch 206/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1995 - acc: 0.9431 - val_loss: 0.1690 - val_acc: 0.9543\n",
      "Epoch 207/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1991 - acc: 0.9430 - val_loss: 0.1688 - val_acc: 0.9543\n",
      "Epoch 208/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.1987 - acc: 0.9434 - val_loss: 0.1684 - val_acc: 0.9545\n",
      "Epoch 209/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1983 - acc: 0.9435 - val_loss: 0.1681 - val_acc: 0.9542\n",
      "Epoch 210/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1979 - acc: 0.9435 - val_loss: 0.1679 - val_acc: 0.9547\n",
      "Epoch 211/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1975 - acc: 0.9437 - val_loss: 0.1676 - val_acc: 0.9543\n",
      "Epoch 212/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1971 - acc: 0.9438 - val_loss: 0.1673 - val_acc: 0.9545\n",
      "Epoch 213/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1968 - acc: 0.9439 - val_loss: 0.1671 - val_acc: 0.9552\n",
      "Epoch 214/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.1964 - acc: 0.9441 - val_loss: 0.1670 - val_acc: 0.9547\n",
      "Epoch 215/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.1960 - acc: 0.9443 - val_loss: 0.1666 - val_acc: 0.9547\n",
      "Epoch 216/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1956 - acc: 0.9442 - val_loss: 0.1662 - val_acc: 0.9547\n",
      "Epoch 217/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.1953 - acc: 0.9444 - val_loss: 0.1661 - val_acc: 0.9548\n",
      "Epoch 218/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.1949 - acc: 0.9446 - val_loss: 0.1657 - val_acc: 0.9560\n",
      "Epoch 219/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.1945 - acc: 0.9443 - val_loss: 0.1654 - val_acc: 0.9552\n",
      "Epoch 220/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1941 - acc: 0.9447 - val_loss: 0.1654 - val_acc: 0.9555\n",
      "Epoch 221/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1938 - acc: 0.9448 - val_loss: 0.1650 - val_acc: 0.9553\n",
      "Epoch 222/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1934 - acc: 0.9448 - val_loss: 0.1647 - val_acc: 0.9560\n",
      "Epoch 223/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.1931 - acc: 0.9449 - val_loss: 0.1646 - val_acc: 0.9555\n",
      "Epoch 224/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1927 - acc: 0.9449 - val_loss: 0.1643 - val_acc: 0.9563\n",
      "Epoch 225/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.1923 - acc: 0.9449 - val_loss: 0.1641 - val_acc: 0.9557\n",
      "Epoch 226/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.1920 - acc: 0.9452 - val_loss: 0.1638 - val_acc: 0.9562\n",
      "Epoch 227/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.1916 - acc: 0.9453 - val_loss: 0.1635 - val_acc: 0.9562\n",
      "Epoch 228/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1913 - acc: 0.9454 - val_loss: 0.1635 - val_acc: 0.9562\n",
      "Epoch 229/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.1909 - acc: 0.9455 - val_loss: 0.1629 - val_acc: 0.9565\n",
      "Epoch 230/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.1906 - acc: 0.9456 - val_loss: 0.1629 - val_acc: 0.9567\n",
      "Epoch 231/256\n",
      "54000/54000 [==============================] - 1s 15us/step - loss: 0.1902 - acc: 0.9456 - val_loss: 0.1628 - val_acc: 0.9563\n",
      "Epoch 232/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1899 - acc: 0.9458 - val_loss: 0.1625 - val_acc: 0.9568\n",
      "Epoch 233/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.1896 - acc: 0.9459 - val_loss: 0.1622 - val_acc: 0.9568\n",
      "Epoch 234/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1892 - acc: 0.9460 - val_loss: 0.1621 - val_acc: 0.9568\n",
      "Epoch 235/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1889 - acc: 0.9461 - val_loss: 0.1617 - val_acc: 0.9573\n",
      "Epoch 236/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.1885 - acc: 0.9462 - val_loss: 0.1615 - val_acc: 0.9572\n",
      "Epoch 237/256\n",
      "54000/54000 [==============================] - 1s 14us/step - loss: 0.1882 - acc: 0.9464 - val_loss: 0.1613 - val_acc: 0.9575\n",
      "Epoch 238/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1878 - acc: 0.9465 - val_loss: 0.1611 - val_acc: 0.9573\n",
      "Epoch 239/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1875 - acc: 0.9466 - val_loss: 0.1608 - val_acc: 0.9577\n",
      "Epoch 240/256\n",
      "54000/54000 [==============================] - 1s 13us/step - loss: 0.1872 - acc: 0.9467 - val_loss: 0.1607 - val_acc: 0.9575\n",
      "Epoch 241/256\n",
      "54000/54000 [==============================] - 3s 57us/step - loss: 0.1868 - acc: 0.9469 - val_loss: 0.1605 - val_acc: 0.9573\n",
      "Epoch 242/256\n",
      "54000/54000 [==============================] - 3s 57us/step - loss: 0.1865 - acc: 0.9468 - val_loss: 0.1602 - val_acc: 0.9575\n",
      "Epoch 243/256\n",
      "54000/54000 [==============================] - 2s 38us/step - loss: 0.1862 - acc: 0.9471 - val_loss: 0.1600 - val_acc: 0.9575\n",
      "Epoch 244/256\n",
      "54000/54000 [==============================] - 2s 35us/step - loss: 0.1859 - acc: 0.9472 - val_loss: 0.1596 - val_acc: 0.9580\n",
      "Epoch 245/256\n",
      "54000/54000 [==============================] - 2s 36us/step - loss: 0.1856 - acc: 0.9471 - val_loss: 0.1596 - val_acc: 0.9580\n",
      "Epoch 246/256\n",
      "54000/54000 [==============================] - ETA: 0s - loss: 0.1854 - acc: 0.947 - 2s 34us/step - loss: 0.1852 - acc: 0.9473 - val_loss: 0.1593 - val_acc: 0.9575\n",
      "Epoch 247/256\n",
      "54000/54000 [==============================] - 2s 35us/step - loss: 0.1849 - acc: 0.9475 - val_loss: 0.1591 - val_acc: 0.9580\n",
      "Epoch 248/256\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.1846 - acc: 0.9476 - val_loss: 0.1592 - val_acc: 0.9582\n",
      "Epoch 249/256\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.1843 - acc: 0.9476 - val_loss: 0.1588 - val_acc: 0.9582\n",
      "Epoch 250/256\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.1840 - acc: 0.9477 - val_loss: 0.1587 - val_acc: 0.9582\n",
      "Epoch 251/256\n",
      "54000/54000 [==============================] - 2s 39us/step - loss: 0.1837 - acc: 0.9477 - val_loss: 0.1585 - val_acc: 0.9578\n",
      "Epoch 252/256\n",
      "54000/54000 [==============================] - 3s 50us/step - loss: 0.1833 - acc: 0.9479 - val_loss: 0.1581 - val_acc: 0.9585\n",
      "Epoch 253/256\n",
      "54000/54000 [==============================] - 3s 52us/step - loss: 0.1830 - acc: 0.9479 - val_loss: 0.1580 - val_acc: 0.9587\n",
      "Epoch 254/256\n",
      "54000/54000 [==============================] - 2s 39us/step - loss: 0.1827 - acc: 0.9480 - val_loss: 0.1577 - val_acc: 0.9585\n",
      "Epoch 255/256\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.1824 - acc: 0.9482 - val_loss: 0.1576 - val_acc: 0.9583\n",
      "Epoch 256/256\n",
      "54000/54000 [==============================] - 1s 23us/step - loss: 0.1821 - acc: 0.9481 - val_loss: 0.1576 - val_acc: 0.9583\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train)= (final_data, final_data_target)\n",
    "(x_test, y_test) = (test_data[0], test_data[1])\n",
    "num_classes=10\n",
    "image_vector_size=28*28\n",
    "x_train = x_train.reshape(x_train.shape[0], image_vector_size)\n",
    "x_test = x_test.reshape(x_test.shape[0], image_vector_size)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "NN_model = Sequential()\n",
    "NN_model.add(Dense(units=32, activation='sigmoid', input_shape=(784,)))\n",
    "NN_model.add(Dense(units=num_classes, activation='softmax'))\n",
    "NN_model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history = NN_model.fit(x_train, y_train, batch_size=128, epochs=256,verbose=1,validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (4, 10000)\n",
      "Implementation 4: Testing Accuracy for Neural Network Using MNIST Dataset:0.9461\n",
      "Implementation 4: Confusion Matrix for Neural Network Using MNIST Dataset:\n",
      "[[ 964    0    9    3    1   10    9    2    5    9]\n",
      " [   0 1116    3    0    2    2    3   10    6    6]\n",
      " [   1    2  968   19    4    3    2   16    3    1]\n",
      " [   1    2    9  950    0   31    1    7   16   13]\n",
      " [   0    0   11    0  931    4    7    5    6   25]\n",
      " [   4    1    1   13    0  808    7    1   15    7]\n",
      " [   6    5    6    1   11    8  926    0    8    1]\n",
      " [   2    2    9   11    2    5    0  964    7   13]\n",
      " [   2    7   14   12    3   14    3    1  904    4]\n",
      " [   0    0    2    1   28    7    0   22    4  930]]\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train)= (final_data, final_data_target)\n",
    "(x_test, y_test) = (test_data[0], test_data[1])\n",
    "x_train = x_train.reshape(x_train.shape[0], image_vector_size)\n",
    "x_test = x_test.reshape(x_test.shape[0], image_vector_size)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "NN_model_pred = NN_model.predict(np.array(x_test))\n",
    "# confusion_matrix_NN1 = confusion_matrix(test_data[1],NN_model_pred)\n",
    "NN_predicted_value = NN_model_pred.argmax(axis=-1)\n",
    "confusion_matrix_NN1 = confusion_matrix(NN_predicted_value,test_data[1])\n",
    "loss,accuracy = NN_model.evaluate(x_test, y_test, verbose=False)\n",
    "majority_voting_list.append(NN_predicted_value)\n",
    "print(\"Majority List: \" + str(np.matrix(majority_voting_list).shape))\n",
    "print(\"Implementation 4: Testing Accuracy for Neural Network Using MNIST Dataset:\" +str(accuracy))\n",
    "print(\"Implementation 4: Confusion Matrix for Neural Network Using MNIST Dataset:\")\n",
    "print(confusion_matrix_NN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 5: Neural Network Using  Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=256, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_Classifier2 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=256, random_state=1)\n",
    "NN_Classifier2.fit(final_data, final_data_target)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (5, 10000)\n",
      "Implementation 5: Testing Accuracy for Neural Network Using MNIST Dataset:0.9785\n",
      "Implementation 5: Confusion Matrix for Neural Network Using MNIST Dataset:\n",
      "[[ 967    0    2    0    0    2    4    1    3    1]\n",
      " [   0 1125    2    1    0    1    2    1    3    0]\n",
      " [   1    0 1007    2    1    0    5    7    8    1]\n",
      " [   2    0    3  989    0    5    0    3    4    4]\n",
      " [   2    0    3    0  958    1    2    2    0   14]\n",
      " [   3    0    0   15    2  861    2    2    3    4]\n",
      " [   4    3    1    1    4    5  936    1    2    1]\n",
      " [   0    4    8    4    1    0    0 1002    6    3]\n",
      " [   4    0    1    3    5    1    1    2  955    2]\n",
      " [   2    2    1    2    6    5    2    3    1  985]]\n"
     ]
    }
   ],
   "source": [
    "NN_pred=NN_Classifier2.predict(test_data[0])\n",
    "y_pred = accuracy_score(test_data[1],NN_pred)\n",
    "majority_voting_list.append(NN_pred)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_list).shape))\n",
    "confusion_matrix_NN2 = confusion_matrix(test_data[1],NN_pred)\n",
    "print(\"Implementation 5: Testing Accuracy for Neural Network Using MNIST Dataset:\"+str(y_pred))\n",
    "print(\"Implementation 5: Confusion Matrix for Neural Network Using MNIST Dataset:\")\n",
    "print(confusion_matrix_NN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 6: Neural Network Using Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=200, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=50, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_Classifier3 = MLPClassifier(solver='sgd', alpha=1e-5,hidden_layer_sizes=200, random_state=1, max_iter=50)\n",
    "NN_Classifier3.fit(final_data, final_data_target)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (6, 10000)\n",
      "Implementation 6: Testing Accuracy for Neural Network Using MNIST Dataset:0.9785\n",
      "Implementation 6: Confusion Matrix for Neural Network Using MNIST Dataset:\n",
      "[[ 965    0    1    1    0    3    7    1    2    0]\n",
      " [   0 1117    3    2    0    1    3    2    7    0]\n",
      " [   7    3  969   12    7    2    6    9   15    2]\n",
      " [   0    1    7  962    0   12    1   12   10    5]\n",
      " [   1    1    3    1  934    0   11    3    5   23]\n",
      " [   7    2    1   27    4  817   11    1   15    7]\n",
      " [  10    3    4    3    8    9  919    1    1    0]\n",
      " [   2    7   20    8    4    1    0  963    1   22]\n",
      " [   6    4    4   14    7    9    8    9  907    6]\n",
      " [   9    7    1   10   20    3    1   11    6  941]]\n"
     ]
    }
   ],
   "source": [
    "NN_pred2=NN_Classifier3.predict(test_data[0])\n",
    "y_pred = accuracy_score(test_data[1],NN_pred)\n",
    "majority_voting_list.append(NN_pred2)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_list).shape))\n",
    "confusion_matrix_NN3 = confusion_matrix(test_data[1],NN_pred2)\n",
    "print(\"Implementation 6: Testing Accuracy for Neural Network Using MNIST Dataset:\"+str(y_pred))\n",
    "print(\"Implementation 6: Confusion Matrix for Neural Network Using MNIST Dataset:\")\n",
    "print(confusion_matrix_NN3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 7: SVM Using kernel as 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=2, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.05, kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = final_data, final_data_target\n",
    "X_test, y_test = test_data[0], test_data[1]\n",
    "SVM_classifier1 = SVC(kernel='linear', C=2, gamma= 0.05);\n",
    "SVM_classifier1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (11, 10000)\n",
      "Implementation 7: Testing Accuracy for SVM Using MNIST Dataset:0.9381\n",
      "Implementation 7: Confusion Matrix for SVM Using MNIST Dataset:\n",
      "[[ 956    0    4    1    1    8    7    1    0    2]\n",
      " [   0 1117    5    3    0    1    2    1    6    0]\n",
      " [   5    7  969   11    2    4    7    9   16    2]\n",
      " [   4    0   15  950    1   19    1    7   11    2]\n",
      " [   1    1    9    0  943    2    5    1    2   18]\n",
      " [  11    5    5   37    6  794   12    2   15    5]\n",
      " [   6    3   14    2    6   18  906    1    2    0]\n",
      " [   2    8   21   12    7    2    0  956    2   18]\n",
      " [   8    5    6   28    7   26    8    6  872    8]\n",
      " [   7    7    3   13   32    5    1   18    5  918]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = test_data[0], test_data[1]\n",
    "SVM_pred1=SVM_classifier1.predict(X_test)\n",
    "y_pred = accuracy_score(test_data[1],SVM_pred1)\n",
    "majority_voting_list.append(SVM_pred1)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_list).shape))\n",
    "confusion_matrix_SVM1 = confusion_matrix(test_data[1],SVM_pred1)\n",
    "print(\"Implementation 7: Testing Accuracy for SVM Using MNIST Dataset:\"+str(y_pred))\n",
    "print(\"Implementation 7: Confusion Matrix for SVM Using MNIST Dataset:\")\n",
    "print(confusion_matrix_SVM1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 8: SVM Using radial basis function and gamma =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = final_data, final_data_target\n",
    "SVM_classifier2 = SVC(kernel='rbf', gamma= 1, max_iter= 30);\n",
    "SVM_classifier2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = test_data[0], test_data[1]\n",
    "SVM_pred2=SVM_classifier2.predict(X_test)\n",
    "y_pred = accuracy_score(test_data[1],SVM_pred2)\n",
    "majority_voting_list.append(SVM_pred2)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_list).shape))\n",
    "confusion_matrix_SVM2 = confusion_matrix(test_data[1],SVM_pred2)\n",
    "print(\"Implementation 8: Testing Accuracy for SVM Using MNIST Dataset:\"+str(y_pred))\n",
    "print(\"Implementation 8: Confusion Matrix for SVM Using MNIST Dataset:\")\n",
    "print(confusion_matrix_SVM2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 9: SVM Using radial basis function and gamma as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=30).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=30, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = final_data, final_data_target\n",
    "SVM_classifier3 = SVC(kernel='rbf', max_iter= 30);\n",
    "SVM_classifier3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (10, 10000)\n",
      "Implementation 9: Testing Accuracy for SVM Using MNIST Dataset:0.4029\n",
      "Implementation 9: Confusion Matrix for SVM Using MNIST Dataset:\n",
      "[[ 965    0    1    1    0    3    7    1    2    0]\n",
      " [   0 1117    3    2    0    1    3    2    7    0]\n",
      " [   7    3  969   12    7    2    6    9   15    2]\n",
      " [   0    1    7  962    0   12    1   12   10    5]\n",
      " [   1    1    3    1  934    0   11    3    5   23]\n",
      " [   7    2    1   27    4  817   11    1   15    7]\n",
      " [  10    3    4    3    8    9  919    1    1    0]\n",
      " [   2    7   20    8    4    1    0  963    1   22]\n",
      " [   6    4    4   14    7    9    8    9  907    6]\n",
      " [   9    7    1   10   20    3    1   11    6  941]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = test_data[0], test_data[1]\n",
    "SVM_pred3=SVM_classifier3.predict(X_test)\n",
    "y_pred = accuracy_score(test_data[1],SVM_pred3)\n",
    "majority_voting_list.append(SVM_pred3)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_list).shape))\n",
    "confusion_matrix_SVM3 = confusion_matrix(test_data[1],SVM_pred3)\n",
    "print(\"Implementation 9: Testing Accuracy for SVM Using MNIST Dataset:\"+str(y_pred))\n",
    "print(\"Implementation 9: Confusion Matrix for SVM Using MNIST Dataset:\")\n",
    "print(confusion_matrix_NN3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 10: SVM Using kernel as `poly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=5, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=20, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = final_data, final_data_target\n",
    "SVM_classifier4 = SVC(kernel='poly', C=5, max_iter=20);\n",
    "SVM_classifier4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (11, 10000)\n",
      "Implementation 10: Testing Accuracy for SVM Using MNIST Dataset:0.1852\n",
      "Implementation 10: Confusion Matrix for SVM Using MNIST Dataset:\n",
      "[[ 533   75  115   50   16   91  202   76  170   18]\n",
      " [   0  294   10    1   20    7    4  110   16   40]\n",
      " [ 229  251 1344  152   47  254  505  203  143  136]\n",
      " [  80  165  122 1283   39  216   67  536  303  432]\n",
      " [ 184  146   29    4  958   24   71   24   78  102]\n",
      " [ 188  181  209  358  208 1224  282  245  694   90]\n",
      " [  59   34   61    7   25   46  769    4   74    8]\n",
      " [ 169  649   44   61  333   64   31  585   86  629]\n",
      " [ 147  142   43   68  184   62   35  166  390  308]\n",
      " [ 411   63   22   16  170   12   34   51   46  237]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = test_data[0], test_data[1]\n",
    "SVM_pred4=SVM_classifier4.predict(X_test)\n",
    "y_pred = accuracy_score(test_data[1],SVM_pred4)\n",
    "majority_voting_list.append(SVM_pred4)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_list).shape))\n",
    "confusion_matrix_SVM4 = confusion_matrix(test_data[1],SVM_pred4)\n",
    "print(\"Implementation 10: Testing Accuracy for SVM Using MNIST Dataset:\"+str(y_pred))\n",
    "print(\"Implementation 10: Confusion Matrix for SVM Using MNIST Dataset:\")\n",
    "print(confusion_matrix_NN4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 11: Random Forest with n_estimators as 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = final_data, final_data_target\n",
    "RF_classifier1 = RandomForestClassifier(n_estimators=10);\n",
    "RF_classifier1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (7, 10000)\n",
      "Implementation 11: Testing Accuracy for Random Forest Using MNIST Dataset:0.9463\n",
      "Implementation 11: Confusion Matrix for Random Forest Using MNIST Dataset:\n",
      "[[ 968    0    0    1    0    4    3    1    2    1]\n",
      " [   0 1122    3    2    0    2    3    0    2    1]\n",
      " [  10    1  970   12    8    3    4   13   10    1]\n",
      " [   0    0   16  955    0   12    0   11   12    4]\n",
      " [   2    0    4    1  935    2    7    1    2   28]\n",
      " [   7    2    3   35    4  815    9    1   10    6]\n",
      " [  10    2    2    1    9    7  923    0    4    0]\n",
      " [   2    8   28    6    6    1    0  960    4   13]\n",
      " [   7    0   11   19    7   17    8    4  892    9]\n",
      " [   7    5    2   13   33    6    0   10   10  923]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = test_data[0], test_data[1]\n",
    "RF_pred1=RF_classifier1.predict(X_test)\n",
    "y_pred = accuracy_score(test_data[1],RF_pred1)\n",
    "majority_voting_list.append(RF_pred1)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_list).shape))\n",
    "confusion_matrix_RF1 = confusion_matrix(test_data[1],RF_pred1)\n",
    "print(\"Implementation 11: Testing Accuracy for Random Forest Using MNIST Dataset:\"+str(y_pred))\n",
    "print(\"Implementation 11: Confusion Matrix for Random Forest Using MNIST Dataset:\")\n",
    "print(confusion_matrix_RF1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 12: Random Forest with n_estimators as 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = final_data, final_data_target\n",
    "RF_classifier2 = RandomForestClassifier(n_estimators=100, criterion= 'entropy');\n",
    "RF_classifier2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (8, 10000)\n",
      "Implementation 12: Testing Accuracy for Random Forest Using MNIST Dataset:0.9698\n",
      "Implementation 12: Confusion Matrix for Random Forest Using MNIST Dataset:\n",
      "[[ 970    1    1    0    0    2    2    1    3    0]\n",
      " [   0 1126    2    1    0    2    2    0    1    1]\n",
      " [   6    0 1000    7    2    0    3    7    6    1]\n",
      " [   0    0    9  971    0    8    0    9    9    4]\n",
      " [   2    0    1    0  952    0    6    0    3   18]\n",
      " [   2    0    1   13    3  859    4    1    6    3]\n",
      " [   6    3    0    0    2    4  941    0    2    0]\n",
      " [   2    4   18    1    2    0    0  987    5    9]\n",
      " [   5    0    4    8    3    6    5    3  931    9]\n",
      " [   4    6    1   11   11    7    1    3    4  961]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = test_data[0], test_data[1]\n",
    "RF_pred2=RF_classifier2.predict(X_test)\n",
    "y_pred = accuracy_score(test_data[1],RF_pred2)\n",
    "majority_voting_list.append(RF_pred2)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_list).shape))\n",
    "confusion_matrix_RF2 = confusion_matrix(test_data[1],RF_pred2)\n",
    "print(\"Implementation 12: Testing Accuracy for Random Forest Using MNIST Dataset:\"+str(y_pred))\n",
    "print(\"Implementation 12: Confusion Matrix for Random Forest Using MNIST Dataset:\")\n",
    "print(confusion_matrix_RF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 13: Random Forest with n_estimators as 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = final_data, final_data_target\n",
    "RF_classifier3 = RandomForestClassifier(n_estimators=200);\n",
    "RF_classifier3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (9, 10000)\n",
      "Implementation 13: Testing Accuracy for Random Forest Using MNIST Dataset:0.971\n",
      "Implementation 13: Confusion Matrix for Random Forest Using MNIST Dataset:\n",
      "[[ 971    0    1    0    0    1    3    1    3    0]\n",
      " [   0 1124    2    3    0    1    3    0    1    1]\n",
      " [   6    0  999    7    2    0    4    9    5    0]\n",
      " [   0    0   11  972    0    5    0    9   10    3]\n",
      " [   1    0    0    0  960    0    5    0    3   13]\n",
      " [   3    0    1   12    4  859    4    2    4    3]\n",
      " [   8    3    0    0    2    1  941    0    3    0]\n",
      " [   1    3   22    0    0    0    0  991    2    9]\n",
      " [   5    0    4    7    5    6    3    3  930   11]\n",
      " [   4    5    2    8   11    3    1    4    8  963]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = test_data[0], test_data[1]\n",
    "RF_pred3=RF_classifier3.predict(X_test)\n",
    "y_pred = accuracy_score(test_data[1],RF_pred3)\n",
    "majority_voting_list.append(RF_pred3)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_list).shape))\n",
    "confusion_matrix_RF3 = confusion_matrix(test_data[1],RF_pred3)\n",
    "print(\"Implementation 13: Testing Accuracy for Random Forest Using MNIST Dataset:\"+str(y_pred))\n",
    "print(\"Implementation 13: Confusion Matrix for Random Forest Using MNIST Dataset:\")\n",
    "print(confusion_matrix_RF3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 14: CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 944s 16ms/step - loss: 0.1455 - acc: 0.9563 - val_loss: 0.0442 - val_acc: 0.9875\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 935s 16ms/step - loss: 0.0408 - acc: 0.9878 - val_loss: 0.0283 - val_acc: 0.9907\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 919s 15ms/step - loss: 0.0274 - acc: 0.9914 - val_loss: 0.0312 - val_acc: 0.9888\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 10621s 177ms/step - loss: 0.0207 - acc: 0.9930 - val_loss: 0.0265 - val_acc: 0.9924\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 918s 15ms/step - loss: 0.0159 - acc: 0.9946 - val_loss: 0.0268 - val_acc: 0.9909\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 941s 16ms/step - loss: 0.0134 - acc: 0.9957 - val_loss: 0.0278 - val_acc: 0.9912\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 928s 15ms/step - loss: 0.0107 - acc: 0.9964 - val_loss: 0.0332 - val_acc: 0.9911\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 920s 15ms/step - loss: 0.0099 - acc: 0.9968 - val_loss: 0.0278 - val_acc: 0.9925\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 910s 15ms/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0374 - val_acc: 0.9901\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 910s 15ms/step - loss: 0.0068 - acc: 0.9978 - val_loss: 0.0323 - val_acc: 0.9919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2364470db70>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "(x_train, y_train)= (final_data, final_data_target)\n",
    "(x_test, y_test) = (test_data[0], test_data[1])\n",
    "img_x, img_y = 28, 28\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "x_train = x_train.reshape(final_data.shape[0], img_x, img_y, 1)\n",
    "x_test = x_test.reshape(test_data[0].shape[0], img_x, img_y, 1)\n",
    "\n",
    "y_train = keras.utils.to_categorical(final_data_target, num_classes)\n",
    "y_test = keras.utils.to_categorical(test_data[1], num_classes)\n",
    "\n",
    "input_shape = (img_x, img_y, 1)\n",
    "\n",
    "CNN_model = Sequential()\n",
    "CNN_model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),activation='relu',input_shape=input_shape))\n",
    "CNN_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "CNN_model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "CNN_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNN_model.add(Flatten())\n",
    "CNN_model.add(Dense(1000, activation='relu'))\n",
    "CNN_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "CNN_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "CNN_model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test),callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ... 4 5 6]\n",
      "Majority List: (10, 10000)\n",
      "Implementation 14: Testing Accuracy for CNN Using MNIST Dataset: 0.9919\n",
      "Implementation 14: Confusion Matrix for CNN Using MNIST Dataset:\n",
      "[[ 974    0    2    0    0    1    1    0    1    2]\n",
      " [   1 1131    3    0    0    0    2    1    0    0]\n",
      " [   1    0 1016    1    0    0    0    2    0    0]\n",
      " [   0    1    2 1007    0    7    0    0    0    0]\n",
      " [   0    0    0    0  979    0    1    1    0   10]\n",
      " [   0    0    0    1    0  878    1    0    0    2]\n",
      " [   2    1    4    0    1    4  951    0    2    0]\n",
      " [   1    2    3    0    0    0    0 1018    0    0]\n",
      " [   1    0    2    1    0    1    2    1  970    0]\n",
      " [   0    0    0    0    2    1    0    5    1  995]]\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train)= (final_data, final_data_target)\n",
    "(x_test, y_test) = (test_data[0], test_data[1])\n",
    "x_train = x_train.reshape(final_data.shape[0], img_x, img_y, 1)\n",
    "x_test = x_test.reshape(test_data[0].shape[0], img_x, img_y, 1)\n",
    "\n",
    "y_train = keras.utils.to_categorical(final_data_target, num_classes)\n",
    "y_test = keras.utils.to_categorical(test_data[1], num_classes)\n",
    "\n",
    "score = CNN_model.evaluate(x_test, y_test, verbose=0)\n",
    "CNN_model_pred = CNN_model.predict(np.array(x_test))\n",
    "CNN_predicted_value = CNN_model_pred.argmax(axis=-1)\n",
    "confusion_matrix_CNN1 = confusion_matrix(CNN_predicted_value,test_data[1])\n",
    "loss,accuracy = CNN_model.evaluate(x_test, y_test, verbose=False)\n",
    "majority_voting_list.append(CNN_predicted_value)\n",
    "print(CNN_predicted_value)\n",
    "print(\"Majority List: \" + str(np.matrix(majority_voting_list).shape))\n",
    "print('Implementation 14: Testing Accuracy for CNN Using MNIST Dataset:', score[1])\n",
    "print(\"Implementation 14: Confusion Matrix for CNN Using MNIST Dataset:\")\n",
    "print(confusion_matrix_CNN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Voting for MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Majority List: (10000,)\n",
      "Testing Accuracy for Majority Voting Using MNIST Dataset: 0.9641\n",
      "Confusion Matrix for Majority Voting  Using MNIST Dataset:\n",
      "[[ 971    0    1    1    0    2    2    1    2    0]\n",
      " [   0 1123    2    1    0    1    3    1    4    0]\n",
      " [   8    2  990    6    3    0    6    8    8    1]\n",
      " [   0    0   10  976    0    8    0    6    8    2]\n",
      " [   1    0    3    0  955    0    6    0    2   15]\n",
      " [   6    1    0   22    4  838    7    1   11    2]\n",
      " [   9    3    2    1    4    7  930    1    1    0]\n",
      " [   2    5   19    2    2    0    0  986    1   11]\n",
      " [   5    2    3   13    6    8    7    5  917    8]\n",
      " [   7    6    1   11   16    2    1    6    4  955]]\n"
     ]
    }
   ],
   "source": [
    "final_majority_list = np.matrix(majority_voting_list)\n",
    "final_majority_list = mode(final_majority_list)\n",
    "# for i in range(len(test_data[1])):\n",
    "#     most_common, num_most_common = Counter(final_majority_list.flat).most_common(1)[0]\n",
    "#     ensemble_list.append(most_common)\n",
    "# print(\"Majority Voting\" +ensemble_list)\n",
    "\n",
    "print(\"Final Majority List: \" +str((final_majority_list[0][0]).shape))\n",
    "y_pred = accuracy_score(test_data[1],final_majority_list[0][0])\n",
    "MV_confusion_matrix = confusion_matrix(test_data[1],final_majority_list[0][0])\n",
    "# print(\"Final Majority List: \" +str(ensemble_list.shape))\n",
    "# y_pred = accuracy_score(test_data[1], ensemble_list)\n",
    "# MV_confusion_matrix = confusion_matrix(test_data[1],ensemble_list)\n",
    "print('Testing Accuracy for Majority Voting Using MNIST Dataset:', str(y_pred))\n",
    "print(\"Confusion Matrix for Majority Voting  Using MNIST Dataset:\")\n",
    "print(MV_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging using Softmax Regression for MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "          random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=True),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=0.8,\n",
       "         max_samples=0.8, n_estimators=5, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import model_selection\n",
    "(x_train, y_train)= (final_data, final_data_target)\n",
    "(x_test, y_test) = (test_data[0], test_data[1])\n",
    "bagging1 = BaggingClassifier(base_estimator=mul_lr2, n_estimators=5, max_samples=0.8, max_features=0.8)\n",
    "bagging1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8976252570443334\n",
      "[[ 955    0    1    3    0    8    7    5    1    0]\n",
      " [   0 1111    4    3    0    1    3    2   11    0]\n",
      " [   3    8  928   20    7    4   15    8   35    4]\n",
      " [   3    1   14  925    0   24    3   11   20    9]\n",
      " [   1    4    7    3  917    0    8    5    9   28]\n",
      " [   9    3    4   36    8  777   14    7   28    6]\n",
      " [   8    3    7    1    7   16  912    2    2    0]\n",
      " [   1    7   26    7    7    1    0  949    2   28]\n",
      " [   9    8    7   24    7   30   11    9  858   11]\n",
      " [   8    7    1   10   24    4    0   20   10  925]]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(bagging1, x_test, y_test, cv=5, scoring='accuracy')\n",
    "print(scores.mean())\n",
    "MV_confusion_matrix = confusion_matrix(test_data[1],bagging1.predict(x_test))\n",
    "print(MV_confusion_matrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9417201414033982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-187-0d3bc304d1f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mMV_confusion_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMV_confusion_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\voting_classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    214\u001b[0m         \"\"\"\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimators_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoting\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'soft'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[0mmaj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "(x_train, y_train)= (final_data, final_data_target)\n",
    "(x_test, y_test) = (test_data[0], test_data[1])\n",
    "estimators = []\n",
    "model1 = mul_lr\n",
    "estimators.append(('Softmax Regression', model1))\n",
    "model2 = mul_lr2\n",
    "estimators.append(('Softmax Regression2', model2))\n",
    "model3 = NN_Classifier2\n",
    "estimators.append(('Neural Network Classifier', model3))\n",
    "model4 = NN_Classifier3\n",
    "estimators.append(('Neural Network Classifier2', model4))\n",
    "model5 = SVM_classifier1\n",
    "estimators.append(('SVM_Classifier1', model5))\n",
    "model6 = RF_classifier1\n",
    "estimators.append(('Random Forest', model6))\n",
    "model7 = RF_classifier2\n",
    "estimators.append(('Random Forest2', model7))\n",
    "model8 = RF_classifier3\n",
    "estimators.append(('Random Forest3', model8))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = model_selection.cross_val_score(ensemble,x_test, y_test, cv=5)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging using neural network for MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging2 = BaggingClassifier(base_estimator=NN_Classifier2, n_estimators=5, max_samples=0.8, max_features=0.8)\n",
    "bagging2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(bagging2, x_test, y_test, cv=5, scoring='accuracy')\n",
    "print(scores.mean())\n",
    "MV_confusion_matrix = confusion_matrix(test_data[1],bagging2.predict(x_test))\n",
    "print(MV_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging using SVM for MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging3 = BaggingClassifier(base_estimator=SVM_classifier1, n_estimators=5, max_samples=0.8, max_features=0.8)\n",
    "bagging3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(bagging3, x_test, y_test, cv=5, scoring='accuracy')\n",
    "print(scores.mean())\n",
    "MV_confusion_matrix = confusion_matrix(test_data[1],bagging3.predict(x_test))\n",
    "print(MV_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging using Random Forest for MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging4 = BaggingClassifier(base_estimator=RF_classifier3, n_estimators=5, max_samples=0.8, max_features=0.8)\n",
    "bagging4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(bagging4, x_test, y_test, cv=5, scoring='accuracy')\n",
    "print(scores.mean())\n",
    "MV_confusion_matrix = confusion_matrix(test_data[1],bagging4.predict(x_test))\n",
    "print(MV_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting using Softmax Regression for MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "          random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=True),\n",
       "          learning_rate=1.0, n_estimators=10, random_state=None)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import model_selection\n",
    "(x_train, y_train)= (final_data, final_data_target)\n",
    "(x_test, y_test) = (test_data[0], test_data[1])\n",
    "boosting1 = AdaBoostClassifier(base_estimator=mul_lr2, n_estimators=10)\n",
    "boosting1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8397181926114442\n",
      "[[ 874    0   12    2    5   64   15    3    1    4]\n",
      " [   0 1106    2    6    0    2    1    1   17    0]\n",
      " [   9   57  724   55   20   11   41    8  102    5]\n",
      " [   7    3   21  877    3   25    4   15   45   10]\n",
      " [   3    1   15    2  881    2    4   18    5   51]\n",
      " [  12    8   10  102   17  542   24    8  118   51]\n",
      " [  10    4   18    1   34   25  860    1    5    0]\n",
      " [   2    7   21   11    5    1    0  911    2   68]\n",
      " [  11   16   17   38    8   28   18   13  806   19]\n",
      " [   4   10    5   11   48    4    0  169   13  745]]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(boosting1, x_test, y_test, cv=5, scoring='accuracy')\n",
    "print(scores.mean())\n",
    "MV_confusion_matrix = confusion_matrix(test_data[1],boosting1.predict(x_test))\n",
    "print(MV_confusion_matrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting using Random Forest for MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          learning_rate=1.0, n_estimators=10, random_state=None)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train)= (final_data, final_data_target)\n",
    "(x_test, y_test) = (test_data[0], test_data[1])\n",
    "boosting2 = AdaBoostClassifier(base_estimator=RF_classifier3, n_estimators=10)\n",
    "boosting2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9474222005346367\n",
      "[[ 970    1    1    0    0    2    2    1    2    1]\n",
      " [   0 1124    2    3    0    2    2    1    1    0]\n",
      " [   6    0 1000    5    3    0    4    8    6    0]\n",
      " [   0    0   11  972    1    7    0    9    8    2]\n",
      " [   1    0    1    0  952    0    4    0    3   21]\n",
      " [   3    0    1   13    3  857    5    1    7    2]\n",
      " [   7    3    1    0    4    5  935    0    3    0]\n",
      " [   1    2   19    3    1    0    0  987    2   13]\n",
      " [   4    0    5    7    2    3    5    5  932   11]\n",
      " [   6    6    3   10   12    3    1    3    4  961]]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(boosting2, x_test, y_test, cv=5, scoring='accuracy')\n",
    "print(scores.mean())\n",
    "MV_confusion_matrix = confusion_matrix(test_data[1],boosting2.predict(x_test))\n",
    "print(MV_confusion_matrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load USPS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19999, 1)\n",
      "(19999, 784)\n"
     ]
    }
   ],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = 'USPS/Numerals'\n",
    "savedImg = []\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)\n",
    "USPSNumeralsTarMat = np.matrix(USPSTar).T\n",
    "USPSNumeralsInputMat = np.matrix(USPSMat)\n",
    "print(USPSNumeralsTarMat.shape)\n",
    "print(USPSNumeralsInputMat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 1)\n",
      "(1500, 784)\n"
     ]
    }
   ],
   "source": [
    "USPSTestMat  = []\n",
    "USPSTestTar  = []\n",
    "curPath  = 'USPS/Test'\n",
    "savedImg = []\n",
    "\n",
    "curFolderPath = curPath + '/'\n",
    "imgs =  os.listdir(curFolderPath)\n",
    "for img in imgs:\n",
    "    curImg = curFolderPath + '/' + img\n",
    "    if curImg[-3:] == 'png':\n",
    "        img = Image.open(curImg,'r')\n",
    "        img = img.resize((28, 28))\n",
    "        savedImg = img\n",
    "        imgdata = (255-np.array(img.getdata()))/255\n",
    "        USPSTestMat.append(imgdata)\n",
    "        USPSTestTar.append(j) \n",
    "USPSTestTar = np.matrix(USPSTestTar).T\n",
    "USPSInputMat = np.matrix(USPSTestMat)\n",
    "print(USPSTestTar.shape)\n",
    "print(USPSInputMat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_voting_usps = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 1: Logistic Regression  using USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (1, 19999)\n",
      "Errors: 12934  Correct :7065\n",
      "Implementation 1: Testing Accuracy for Softmax Regression Using USPS Dataset: 35.326766338316915\n",
      "Implementation 1: Confusion Matrix for Softmax Regression Using USPS Dataset:\n",
      "[[ 622    4  384   51  294  108   87   42  108  300]\n",
      " [ 228  331  133  318  292   59   44  325  255   15]\n",
      " [ 216   31 1158  134   74   88   97  105   73   23]\n",
      " [ 112    4  122 1233   29  248   32   68   85   67]\n",
      " [  57   93   35   54 1071  136   38  132  247  137]\n",
      " [ 181   20  205  169   50 1074  125   75   69   32]\n",
      " [ 370   14  350   94  108  244  693   30   63   34]\n",
      " [ 177  244  336  405   79  113   34  319  242   51]\n",
      " [ 218   34  154  200  143  596  118   44  405   88]\n",
      " [  50  202  176  439  164   97   15  385  313  159]]\n"
     ]
    }
   ],
   "source": [
    "right=0\n",
    "wrong=0\n",
    "y_pred = make_prediction(np.array(USPSNumeralsInputMat), W)\n",
    "\n",
    "majority_voting_usps.append(y_pred)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_usps).shape))\n",
    "for i,j in zip(y_pred,USPSNumeralsTarMat): \n",
    "    if j == i:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "print(\"Implementation 1: Testing Accuracy for Softmax Regression Using USPS Dataset: \" + str(right/(right+wrong)*100))\n",
    "print(\"Implementation 1: Confusion Matrix for Softmax Regression Using USPS Dataset:\")\n",
    "print(confusion_matrix(USPSNumeralsTarMat, (y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 2 : Logistic Regression  using USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (2, 19999)\n",
      "Implementation 2: Testing Accuracy for Softmax Regression Using USPS Dataset:0.31766588329416473\n",
      "Implementation 2: Confusion Matrix for Softmax Regression Using USPS Dataset:\n",
      "[[ 390    1  200  149   70  301   31  595   84  179]\n",
      " [  37  264  400  134  266  239   18  499  116   27]\n",
      " [  46   22 1253  114   23  369   68   41   41   22]\n",
      " [  29    6  281  861   10  671    8   74   42   18]\n",
      " [  29    9   84   40  660  184   20  727  143  104]\n",
      " [  45    5  364  210   19 1191   24   86   44   12]\n",
      " [  78    5  728   82   43  427  558   24    9   46]\n",
      " [  74   45  108  536   42  188    8  789  161   49]\n",
      " [ 172    7  191  501   63  565   76  182  196   47]\n",
      " [  15   10  105  501   70   87    6  825  190  191]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = accuracy_score(USPSNumeralsTarMat, mul_lr.predict(USPSNumeralsInputMat))\n",
    "confusion_matrix_logisticReg2 = confusion_matrix(USPSNumeralsTarMat, mul_lr.predict(USPSNumeralsInputMat))\n",
    "majority_voting_usps.append(mul_lr.predict(USPSNumeralsInputMat))\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_usps).shape))\n",
    "print('Implementation 2: Testing Accuracy for Softmax Regression Using USPS Dataset:'+str(y_pred))\n",
    "print(\"Implementation 2: Confusion Matrix for Softmax Regression Using USPS Dataset:\")\n",
    "print(confusion_matrix_logisticReg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 3 : Logistic Regression  using USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (3, 19999)\n",
      "Implementation 3: Testing Accuracy for Softmax Regression Using USPS Dataset:0.31406570328516425\n",
      "Implementation 3: Confusion Matrix for Softmax Regression Using USPS Dataset:\n",
      "[[ 371    1  204  153   75  297   33  638   76  152]\n",
      " [  36  251  410  130  284  240   18  478  124   29]\n",
      " [  47   24 1244  113   21  374   70   41   46   19]\n",
      " [  27    5  287  837   12  681    8   78   49   16]\n",
      " [  29    5   84   43  638  182   19  763  135  102]\n",
      " [  41    5  380  214   21 1168   24   89   44   14]\n",
      " [  73    6  731   88   40  418  559   26   12   47]\n",
      " [  71   41   98  520   39  177    6  833  166   49]\n",
      " [ 177    6  191  508   66  539   76  199  194   44]\n",
      " [  14    7  106  493   68   85    3  863  175  186]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = accuracy_score(USPSNumeralsTarMat, mul_lr2.predict(USPSNumeralsInputMat))\n",
    "confusion_matrix_logisticReg3 = confusion_matrix(USPSNumeralsTarMat, mul_lr2.predict(USPSNumeralsInputMat))\n",
    "majority_voting_usps.append(mul_lr2.predict(USPSNumeralsInputMat))\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_usps).shape))\n",
    "print('Implementation 3: Testing Accuracy for Softmax Regression Using USPS Dataset:'+str(y_pred))\n",
    "print(\"Implementation 3: Confusion Matrix for Softmax Regression Using USPS Dataset:\")\n",
    "print(confusion_matrix_logisticReg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 4: Neural Network using USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19999, 10)\n",
      "(19999,)\n",
      "Majority List: (1, 14)\n",
      "Implementation 4: Testing Accuracy for Neural Network Using USPS Dataset:0.3808690434506824\n",
      "Implementation 4: Confusion Matrix for Neural Network Using USPS Dataset:\n",
      "[[ 533   75  115   50   16   91  202   76  170   18]\n",
      " [   0  294   10    1   20    7    4  110   16   40]\n",
      " [ 229  251 1344  152   47  254  505  203  143  136]\n",
      " [  80  165  122 1283   39  216   67  536  303  432]\n",
      " [ 184  146   29    4  958   24   71   24   78  102]\n",
      " [ 188  181  209  358  208 1224  282  245  694   90]\n",
      " [  59   34   61    7   25   46  769    4   74    8]\n",
      " [ 169  649   44   61  333   64   31  585   86  629]\n",
      " [ 147  142   43   68  184   62   35  166  390  308]\n",
      " [ 411   63   22   16  170   12   34   51   46  237]]\n"
     ]
    }
   ],
   "source": [
    "(x_test, y_test) = (np.array(USPSNumeralsInputMat),USPSNumeralsTarMat)\n",
    "x_test = x_test.reshape(x_test.shape[0], image_vector_size )\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "NN_model_pred = NN_model.predict(np.array(x_test))\n",
    "print(NN_model_pred.shape)\n",
    "NN_predicted_value2 = NN_model_pred.argmax(axis=-1)\n",
    "print(NN_predicted_value2.shape)\n",
    "confusion_matrix_NN4 = confusion_matrix(NN_predicted_value2,USPSNumeralsTarMat)\n",
    "loss,accuracy = NN_model.evaluate(x_test, y_test, verbose=False)\n",
    "majority_voting_list.append(NN_predicted_value2)\n",
    "print(\"Majority List: \" + str(np.matrix(majority_voting_list).shape))\n",
    "print(\"Implementation 4: Testing Accuracy for Neural Network Using USPS Dataset:\" +str(accuracy))\n",
    "print(\"Implementation 4: Confusion Matrix for Neural Network Using USPS Dataset:\")\n",
    "print(confusion_matrix_NN4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 5 & 6: Neural Network using USPS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (4, 19999)\n",
      "Implementation 5: Testing Accuracy for Neural Network Using USPS Dataset:0.4856242812140607\n",
      "Implementation 5: Confusion Matrix for Neural Network Using USPS Dataset:\n",
      "[[ 583    1   86   65  428   59   30  183  177  388]\n",
      " [  50  523  366   48  348   40   34  349  179   63]\n",
      " [  63   15 1498   83   38  100   49   43  100   10]\n",
      " [  30   13  139 1383   21  259    5   28   91   31]\n",
      " [   9   21   44   11 1102   51   15  376  267  104]\n",
      " [  43    9  104  131   37 1342   26   72  199   37]\n",
      " [  96   25  316   62   71  120 1061   42  130   77]\n",
      " [  44  153  219  205   65   26    6  985  277   20]\n",
      " [ 116   13  112  244  127  190   55  183  919   41]\n",
      " [  12   32   92  115  222   19    3  742  447  316]]\n",
      "Majority List: (5, 19999)\n",
      "Implementation 6: Testing Accuracy for Neural Network Using USPS Dataset:0.4856242812140607\n",
      "Implementation 6: Confusion Matrix for Neural Network Using USPS Dataset:\n",
      "[[ 517    2  330   78  284  192   81   64   82  370]\n",
      " [ 107  341  266  209   93  146   25  721   68   24]\n",
      " [ 101   10 1439  134   36  127   43   61   32   16]\n",
      " [  39    2  167 1403    7  247    5   60   55   15]\n",
      " [  26   37   83   36  987  181   30  301  183  136]\n",
      " [  84   11  246  156   30 1276   59   49   77   12]\n",
      " [ 215    7  497   72   73  324  721   24   37   30]\n",
      " [ 106  149  329  487   30  191    7  529  139   33]\n",
      " [ 172   12  194  272  110  622   98   71  404   45]\n",
      " [  20   92  153  451  120   76    8  565  313  202]]\n"
     ]
    }
   ],
   "source": [
    "NN_pred= NN_Classifier2.predict(USPSNumeralsInputMat)\n",
    "y_pred = accuracy_score(USPSNumeralsTarMat,NN_pred)\n",
    "majority_voting_usps.append(NN_pred)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_usps).shape))\n",
    "confusion_matrix_NN2 = confusion_matrix(USPSNumeralsTarMat,NN_pred)\n",
    "print('Implementation 5: Testing Accuracy for Neural Network Using USPS Dataset:'+str(y_pred))\n",
    "print(\"Implementation 5: Confusion Matrix for Neural Network Using USPS Dataset:\")\n",
    "print(confusion_matrix_NN2)\n",
    "\n",
    "NN_pred2=NN_Classifier3.predict(USPSNumeralsInputMat)\n",
    "y_pred = accuracy_score(USPSNumeralsTarMat,NN_pred)\n",
    "majority_voting_usps.append(NN_pred2)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_usps).shape))\n",
    "confusion_matrix_NN3 = confusion_matrix(USPSNumeralsTarMat,NN_pred2)\n",
    "print('Implementation 6: Testing Accuracy for Neural Network Using USPS Dataset:'+str(y_pred))\n",
    "print(\"Implementation 6: Confusion Matrix for Neural Network Using USPS Dataset:\")\n",
    "print(confusion_matrix_NN3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 7: SVM using USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (10, 19999)\n",
      "Implementation 7: Testing Accuracy for SVM Using USPS Dataset:0.28451422571128554\n",
      "Implementation 7: Confusion Matrix for SVM Using USPS Dataset:\n",
      "[[ 360    1  528  168  156  321   77  194    8  187]\n",
      " [  46  294  560  205  253  204   20  358   44   16]\n",
      " [ 118   70 1269  104   35  261   67   40   22   13]\n",
      " [  50   60  390  795    9  588    8   53   34   13]\n",
      " [  23   23  249   98  765  216   19  456   87   64]\n",
      " [  37   17  699  218   29  892   30   32   33   13]\n",
      " [ 144   20  853   63   68  338  443   32    2   37]\n",
      " [  27   75  218  639   49  274   11  575  102   30]\n",
      " [ 115   14  360  466   90  655   65   65  142   28]\n",
      " [  11   31  212  608  122  100    4  606  151  155]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = USPSNumeralsInputMat, USPSNumeralsTarMat\n",
    "SVM_pred1=SVM_classifier1.predict(X_test)\n",
    "y_pred = accuracy_score(y_test,SVM_pred1)\n",
    "majority_voting_usps.append(SVM_pred1)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_usps).shape))\n",
    "confusion_matrix_SVM1 = confusion_matrix(y_test,SVM_pred1)\n",
    "print('Implementation 7: Testing Accuracy for SVM Using USPS Dataset:'+str(y_pred))\n",
    "print(\"Implementation 7: Confusion Matrix for SVM Using USPS Dataset:\")\n",
    "print(confusion_matrix_SVM1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 8 & 9: SVM using USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = USPSNumeralsInputMat, USPSNumeralsTarMat\n",
    "SVM_pred2=SVM_classifier2.predict(X_test)\n",
    "y_pred = accuracy_score(y_test,SVM_pred2)\n",
    "majority_voting_usps.append(SVM_pred2)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_usps).shape))\n",
    "confusion_matrix_SVM2 = confusion_matrix(y_test,SVM_pred2)\n",
    "print('Implementation 8: Testing Accuracy for SVM Using USPS Dataset:'+str(y_pred))\n",
    "print(\"Implementation 8: Confusion Matrix for SVM Using USPS Dataset:\")\n",
    "print(confusion_matrix_SVM2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = USPSNumeralsInputMat, USPSNumeralsTarMat\n",
    "SVM_pred3=SVM_classifier3.predict(X_test)\n",
    "y_pred = accuracy_score(y_test,SVM_pred3)\n",
    "majority_voting_usps.append(SVM_pred3)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_usps).shape))\n",
    "confusion_matrix_SVM3 = confusion_matrix(y_test,SVM_pred3)\n",
    "print('Implementation 9: Testing Accuracy for SVM Using USPS Dataset:'+str(y_pred))\n",
    "print(\"Implementation 9: Confusion Matrix for SVM Using USPS Dataset:\")\n",
    "print(confusion_matrix_SVM3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = USPSNumeralsInputMat, USPSNumeralsTarMat\n",
    "SVM_pred4=SVM_classifier4.predict(X_test)\n",
    "y_pred = accuracy_score(y_test,SVM_pred4)\n",
    "majority_voting_usps.append(SVM_pred4)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_usps).shape))\n",
    "confusion_matrix_SVM4 = confusion_matrix(y_test,SVM_pred4)\n",
    "print('Implementation 10: Testing Accuracy for SVM Using USPS Dataset:'+str(y_pred))\n",
    "print(\"Implementation 10: Confusion Matrix for SVM Using USPS Dataset:\")\n",
    "print(confusion_matrix_SVM4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 10 & 11 & 12: Random Forest using USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (6, 19999)\n",
      "Implementation 10: Testing Accuracy for Random Forest Using USPS Dataset:0.32301615080754037\n",
      "Implementation 10: Confusion Matrix for Random Forest Using USPS Dataset:\n",
      "[[ 683   53  328   74  311  131   92  123   13  192]\n",
      " [  63  736  137  118  104  123   32  668   11    8]\n",
      " [ 192  159  936  167   64  178   42  223   16   22]\n",
      " [  92  118  218 1014   66  282   14  131   21   44]\n",
      " [  53  282  120  100  888  145   33  289   31   59]\n",
      " [ 205  102  170  264   80  947   60  107   28   37]\n",
      " [ 429  110  296  114  128  291  500   91   18   23]\n",
      " [  71  496  429  207   43  168   37  515   15   19]\n",
      " [ 173  129  265  277  165  638  106   71  121   55]\n",
      " [  52  349  289  333  238  152   32  361   74  120]]\n",
      "Majority List: (7, 19999)\n",
      "Implementation 11: Testing Accuracy for Random Forest Using USPS Dataset:0.38906945347267363\n",
      "Implementation 11: Confusion Matrix for Random Forest Using USPS Dataset:\n",
      "[[ 607   17  215   54  489  152   75  152    4  235]\n",
      " [  24  604   74   99   64   86   17 1015   15    2]\n",
      " [  77   48 1175   84   64  184   20  341    5    1]\n",
      " [  30   18   67 1257   61  314    3  232    3   15]\n",
      " [   9  223   37   23 1098  154    9  401   22   24]\n",
      " [ 137   34  112   85   33 1407   16  162    5    9]\n",
      " [ 314   70  213   27  113  374  698  182    3    6]\n",
      " [  46  381  299  222   46  272   22  703    0    9]\n",
      " [  50   74  146  186  118 1069   62  119  150   26]\n",
      " [  10  301  199  274  269  131    7  678   49   82]]\n",
      "Majority List: (8, 19999)\n",
      "Implementation 12: Testing Accuracy for Random Forest Using USPS Dataset:0.4032701635081754\n",
      "Implementation 12: Confusion Matrix for Random Forest Using USPS Dataset:\n",
      "[[ 619    9  277   52  461  164   74   95    2  247]\n",
      " [  30  570  124  119   62   91   22  964   17    1]\n",
      " [  78   28 1277   79   66  209   14  243    4    1]\n",
      " [  34    9   87 1293   58  316    5  175    4   19]\n",
      " [  11  209   49   25 1090  166   21  380   22   27]\n",
      " [ 117   27  136   79   25 1478   20  102   10    6]\n",
      " [ 281   47  239   21   98  378  797  121    5   13]\n",
      " [  33  323  396  252   37  241   29  678    6    5]\n",
      " [  46   49  151  215  112 1087   59   95  161   25]\n",
      " [  17  279  255  299  250  134    8  571   85  102]]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = USPSNumeralsInputMat, USPSNumeralsTarMat\n",
    "\n",
    "RF_pred1=RF_classifier1.predict(X_test)\n",
    "y_pred = accuracy_score(y_test,RF_pred1)\n",
    "majority_voting_usps.append(RF_pred1)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_usps).shape))\n",
    "confusion_matrix_RF1 = confusion_matrix(y_test,RF_pred1)\n",
    "print('Implementation 10: Testing Accuracy for Random Forest Using USPS Dataset:'+str(y_pred))\n",
    "print(\"Implementation 10: Confusion Matrix for Random Forest Using USPS Dataset:\")\n",
    "print(confusion_matrix_RF1)\n",
    "\n",
    "RF_pred2=RF_classifier2.predict(X_test)\n",
    "y_pred = accuracy_score(y_test,RF_pred2)\n",
    "majority_voting_usps.append(RF_pred2)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_usps).shape))\n",
    "confusion_matrix_RF2 = confusion_matrix(y_test,RF_pred2)\n",
    "print('Implementation 11: Testing Accuracy for Random Forest Using USPS Dataset:'+str(y_pred))\n",
    "print(\"Implementation 11: Confusion Matrix for Random Forest Using USPS Dataset:\")\n",
    "print(confusion_matrix_RF2)\n",
    "\n",
    "RF_pred3=RF_classifier3.predict(X_test)\n",
    "y_pred = accuracy_score(y_test,RF_pred3)\n",
    "majority_voting_usps.append(RF_pred3)\n",
    "print(\"Majority List: \" +  str(np.matrix(majority_voting_usps).shape))\n",
    "confusion_matrix_RF3 = confusion_matrix(y_test,RF_pred3)\n",
    "print('Implementation 12: Testing Accuracy for Random Forest Using USPS Dataset:'+str(y_pred))\n",
    "print(\"Implementation 12: Confusion Matrix for Random Forest Using USPS Dataset:\")\n",
    "print(confusion_matrix_RF3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 13: CNN on USPS data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority List: (9, 19999)\n",
      "Implementation 13: Testing Accuracy for CNN Using USPS Dataset: 0.7334366718484936\n",
      "Implementation 13: Confusion Matrix for CNN Using USPS Dataset:\n",
      "[[1009   25   11    1    1    6   66    6   32    2]\n",
      " [   4 1094    1    2   53    4   17  154   13   71]\n",
      " [  24   96 1774   20   14   29   53  200   31   30]\n",
      " [  11   56  115 1863    3   27    3  347   49   69]\n",
      " [ 180  480    9    4 1711   17   32   50   74  230]\n",
      " [   2    5   28   96   25 1835    9   37  253   19]\n",
      " [  36   51   12    2    3    3 1775    2   40    1]\n",
      " [   3  156   20    2   56   24    0 1092   13  232]\n",
      " [  15   23   23    5   97    9   41   74 1372  203]\n",
      " [ 716   14    6    5   37   46    4   38  123 1143]]\n"
     ]
    }
   ],
   "source": [
    "(x_test, y_test) = (np.array(USPSNumeralsInputMat),USPSNumeralsTarMat)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_test = x_test.reshape(USPSNumeralsInputMat.shape[0], img_x, img_y, 1)\n",
    "score = CNN_model.evaluate(x_test, y_test, verbose=0)\n",
    "CNN_model_pred_usps = CNN_model.predict(np.array(x_test))\n",
    "CNN_predicted_value_usps = CNN_model_pred_usps.argmax(axis=-1)\n",
    "confusion_matrix_CNN2 = confusion_matrix(CNN_predicted_value_usps,USPSNumeralsTarMat)\n",
    "loss,accuracy = CNN_model.evaluate(x_test, y_test, verbose=False)\n",
    "majority_voting_usps.append(CNN_predicted_value_usps)\n",
    "print(\"Majority List: \" + str(np.matrix(majority_voting_usps).shape))\n",
    "print('Implementation 13: Testing Accuracy for CNN Using USPS Dataset:', score[1])\n",
    "print(\"Implementation 13: Confusion Matrix for CNN Using USPS Dataset:\")\n",
    "print(confusion_matrix_CNN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Voting on USPS data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Majority USPS List: (19999,)\n",
      "Testing Accuracy for Majority Voting Using USPS Dataset: 0.43572178608930445\n",
      "Confusion Matrix for Majority Voting  Using USPS Dataset:\n",
      "[[ 683    4  315   65  330  147   35  123   20  278]\n",
      " [  60  521  280  158  170  128   19  615   43    6]\n",
      " [  87   15 1516   80   30  154   28   68   16    5]\n",
      " [  37    2  160 1432    8  279    4   47   23    8]\n",
      " [  14  104   58   27 1178  151   11  310  108   39]\n",
      " [  74   12  243  131   19 1430   11   54   21    5]\n",
      " [ 211   21  509   54   67  302  785   26    5   20]\n",
      " [  63  220  274  417   30  191    7  689   93   16]\n",
      " [ 148   19  175  335   83  739   70   89  317   25]\n",
      " [  17  161  158  425  135   81    4  648  208  163]]\n"
     ]
    }
   ],
   "source": [
    "final_majority_usps_list = np.matrix(majority_voting_usps)\n",
    "final_majority_usps_list = mode(final_majority_usps_list)\n",
    "print(\"Final Majority USPS List: \" +str((final_majority_usps_list[0][0]).shape))\n",
    "y_pred = accuracy_score(y_test,final_majority_usps_list[0][0])\n",
    "MV1_confusion_matrix = confusion_matrix(y_test,final_majority_usps_list[0][0])\n",
    "print('Testing Accuracy for Majority Voting Using USPS Dataset:', str(y_pred))\n",
    "print(\"Confusion Matrix for Majority Voting  Using USPS Dataset:\")\n",
    "print(MV1_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging for USPS Dataset Using Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7655876719179795\n",
      "[[ 386    1  236  166   89  276   39  592   46  169]\n",
      " [  36  270  393  161  251  260   20  459  124   26]\n",
      " [  37   27 1242  129   24  375   74   41   29   21]\n",
      " [  29    5  313  840   14  662    7   82   37   11]\n",
      " [  23    9   98   56  676  213   19  692  126   88]\n",
      " [  41    5  411  212   25 1159   25   88   24   10]\n",
      " [  91    5  699   97   39  443  544   22    9   51]\n",
      " [  92   52  121  513   56  175   10  802  136   43]\n",
      " [ 200    7  236  503   79  515   71  190  151   48]\n",
      " [  14   12  112  534   73   92    3  793  180  187]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8837428107026757\n",
      "[[ 653    0  149   64  294   60   33  145  199  403]\n",
      " [  67  543  324   57  258   29   30  522  119   51]\n",
      " [  58   11 1555   80   27   87   47   37   88    9]\n",
      " [  39    6  131 1481   10  221    2   33   62   15]\n",
      " [  12   21   48   19 1203   40   11  321  264   61]\n",
      " [  60    4  143  121   32 1342   27   59  198   14]\n",
      " [ 139   19  317   53   66   95 1140   15   89   67]\n",
      " [  58  113  234  225   65   27   13  956  293   16]\n",
      " [ 155    7  151  258  103  245   68  107  871   35]\n",
      " [  16   21  113  213  171   17    1  748  440  260]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7934386846711678\n",
      "[[2000    0    0    0    0    0    0    0    0    0]\n",
      " [2000    0    0    0    0    0    0    0    0    0]\n",
      " [1999    0    0    0    0    0    0    0    0    0]\n",
      " [2000    0    0    0    0    0    0    0    0    0]\n",
      " [2000    0    0    0    0    0    0    0    0    0]\n",
      " [2000    0    0    0    0    0    0    0    0    0]\n",
      " [2000    0    0    0    0    0    0    0    0    0]\n",
      " [2000    0    0    0    0    0    0    0    0    0]\n",
      " [2000    0    0    0    0    0    0    0    0    0]\n",
      " [2000    0    0    0    0    0    0    0    0    0]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bagging4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-178-c56c668bdc3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMV_confusion_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbagging4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mMV_confusion_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbagging4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bagging4' is not defined"
     ]
    }
   ],
   "source": [
    "x_test, y_test = (np.array(USPSNumeralsInputMat), USPSNumeralsTarMat)\n",
    "\n",
    "scores = cross_val_score(bagging1, x_test, y_test, cv=5, scoring='accuracy')\n",
    "print(scores.mean())\n",
    "MV_confusion_matrix = confusion_matrix(y_test,bagging1.predict(x_test))\n",
    "print(MV_confusion_matrix) \n",
    "\n",
    "scores = cross_val_score(bagging2, x_test, y_test, cv=5, scoring='accuracy')\n",
    "print(scores.mean())\n",
    "MV_confusion_matrix = confusion_matrix(y_test,bagging2.predict(x_test))\n",
    "print(MV_confusion_matrix) \n",
    "\n",
    "scores = cross_val_score(bagging3, x_test, y_test, cv=5, scoring='accuracy')\n",
    "print(scores.mean())\n",
    "MV_confusion_matrix = confusion_matrix(y_test,bagging3.predict(x_test))\n",
    "print(MV_confusion_matrix) \n",
    "\n",
    "scores = cross_val_score(bagging4, x_test, y_test, cv=5, scoring='accuracy')\n",
    "print(scores.mean())\n",
    "MV_confusion_matrix = confusion_matrix(y_test,bagging4.predict(x_test))\n",
    "print(MV_confusion_matrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting for USPS Dataset Using Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6778325206301575\n",
      "[[152   0 420 175 107 325 111 376  46 288]\n",
      " [ 22 206 463 159 265 209  21 543 103   9]\n",
      " [121 113 950 155  39 300 116  61 122  22]\n",
      " [ 46  65 315 990  13 311  11  96 124  29]\n",
      " [ 10   4 185 113 683 150  22 567 129 137]\n",
      " [ 37  13 595 337  29 667  47  67 124  84]\n",
      " [102  26 854  90  86 369 378  35   9  51]\n",
      " [  8  36  90 700  40 199  20 674 126 107]\n",
      " [ 64  14 202 496 135 584  67 141 161 136]\n",
      " [  7  17 105 670  76  77   8 587 146 307]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\MS\\1stSem\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.852340897724431\n",
      "[[ 643    9  276   58  428  149   65  107    2  263]\n",
      " [  43  572  109  112   53   85   24  987   14    1]\n",
      " [  93   28 1256   63   60  197   20  271    9    2]\n",
      " [  41    4   88 1284   56  333    2  173    4   15]\n",
      " [  12  184   55   40 1091  168   14  400   20   16]\n",
      " [ 135   23  134   70   31 1447   28  116   11    5]\n",
      " [ 308   53  206   25   91  356  828  121    4    8]\n",
      " [  33  310  397  250   40  222   26  707    9    6]\n",
      " [  50   45  165  212  106 1056   63  102  180   21]\n",
      " [  18  242  241  309  238  133   10  611   89  109]]\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = (np.array(USPSNumeralsInputMat), USPSNumeralsTarMat)\n",
    "\n",
    "scores = cross_val_score(boosting1, x_test, y_test, cv=5, scoring='accuracy')\n",
    "print(scores.mean())\n",
    "MV_confusion_matrix = confusion_matrix(y_test,boosting1.predict(x_test))\n",
    "print(MV_confusion_matrix) \n",
    "\n",
    "\n",
    "scores = cross_val_score(boosting2, x_test, y_test, cv=5, scoring='accuracy')\n",
    "print(scores.mean())\n",
    "MV_confusion_matrix = confusion_matrix(y_test,boosting2.predict(x_test))\n",
    "print(MV_confusion_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
